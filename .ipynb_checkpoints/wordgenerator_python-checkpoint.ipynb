{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np     \n",
    "import matplotlib.pyplot as plt \n",
    "import random  \n",
    "#import string    #string.ascii_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creer de la generecite pour l'ordre de la chaine de markov\n",
    "\n",
    "max_ord = 256 #acceptance threshold for unicode code\n",
    "markov_order = 1 #ordre de la chaine de markov (pour l'instant ne marche qu'avec 1, a generaliser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice des fréquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_passage = np.zeros([max_ord]*(markov_order+1), dtype = \"int32\")\n",
    "First = np.zeros(max_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture fichier source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon = \"pokemon_names.txt\"\n",
    "german = \"german_words.txt\"\n",
    "test = \"text.txt\"\n",
    "mobydick = \"mobydick.txt\"\n",
    "\n",
    "choice = pokemon\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist] #function to make a list of lists flatten\n",
    "\n",
    "def is_ascii(s,lim):\n",
    "    \"\"\"Test if the word s contains a character with a unicode code bigger than lim\"\"\"\n",
    "    return all(ord(c) < lim for c in s)\n",
    "\n",
    "def only_ascii(word,lim):\n",
    "    \"\"\"exclude non desired ascii characters from words\"\"\"\n",
    "    return ''.join([l for l in word if ord(l) < lim])\n",
    "\n",
    "def only_alpha(word):\n",
    "    \"\"\"exclude non  alpha ascii characters from words\"\"\"\n",
    "    return ''.join([l for l in word if l.isalpha()])\n",
    "\n",
    "\n",
    "#Voir plus tard comment gerer les problemes d'encodage de certain textes avec codecs\n",
    "with open (choice, \"r\") as myfile:\n",
    "    words = [only_alpha(w) for line in myfile.read().splitlines() for w in line.split(\" \") if len(only_alpha(w))>0]\n",
    "        \n",
    "#words = [w for w in words if is_ascii(w,max_ord)]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remplissage des fréquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words :\n",
    "    for i,l in enumerate(w):\n",
    "        if i==0: \n",
    "            First[ord(l)] += 1\n",
    "        else:\n",
    "            mat_passage[ord(w[i-1]),ord(l)] += 1\n",
    "            \n",
    "count = mat_passage.sum(axis=1)\n",
    "st = np.tile(count,len(count)).reshape(-1,len(count)).T\n",
    "p = mat_passage.astype('float')/st\n",
    "p[np.isnan(p)]=0\n",
    "\n",
    "#Pour analyser les occurences de symboles : {chr(i):j for i,j in enumerate(count)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation : Ne marche pas pour le moment !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(mat_passage, cmap=\"plasma\")\n",
    "plt.yticks(np.arange(0.5, maxi_size, 1), [chr(c) for c in range(maxi_size)])\n",
    "plt.xticks(np.arange(0.5, maxi_size, 1), [chr(c) for c in range(maxi_size)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(maxi_len):\n",
    "    \n",
    "    new_word = chr(random.choices(range(max_ord),weights=First, k=1)[0])\n",
    "\n",
    "    for i in range(maxi_len-1):\n",
    "        new_word += chr(random.choices(range(max_ord),weights=mat_passage[ord(new_word[i])], k=1)[0])\n",
    "    return new_word.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi_size = 10\n",
    "rep = 5\n",
    "\n",
    "for m in range(3,maxi_size+1):\n",
    "    for n in range(rep):\n",
    "        print(generate(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amelioration\n",
    "\n",
    "- A partir de n'importe quel .txt, lire le fichier mot par mot, exclure les mots inutiles (techniques usuelles nlp ?)\n",
    "- Augmenter l'ordre de la chaine de Markov\n",
    "- Faire un interface plus interacrive\n",
    "- Ameliorer visualisation\n",
    "- generer du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification des dictionnaires science etonnante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "dic = \"FR.txt\"\n",
    "def clean_up(dic):\n",
    "    '''Return a list with all words in appropriate format'''\n",
    "    with open (\"./data/{}\".format(dic), \"r\") as myfile:\n",
    "        words =  myfile.read().splitlines()\n",
    "    reg = '^[a-zA-Z0-9_]+'\n",
    "    words2 = [re.findall(reg, w) for w in words if re.findall(reg, w)]\n",
    "    words2 = [w[0] for w in words2 if len(w[0])>1]\n",
    "    \n",
    "    return words2\n",
    "\n",
    "def write_new_dict(words, name):\n",
    "    '''Takes the ouput of clean_up and write the words in a new file'''\n",
    "    file = open(\"./data/{}\".format(name),\"w\") \n",
    "    for w in words:\n",
    "        file.write(w)\n",
    "        file.write(\"\\n\")\n",
    "    file.close() \n",
    "    \n",
    "    return \"Done\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = clean_up(dic)\n",
    "write_new_dict(words, \"NewFR.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
